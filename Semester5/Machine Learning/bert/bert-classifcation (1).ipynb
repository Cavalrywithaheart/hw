{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport csv\nimport codecs\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nx_train = np.array(pd.read_csv('../input/hust-ml/train1.csv', dtype=str, encoding='utf-8', usecols=[0])).ravel().tolist()\ntrain_label = np.array(pd.read_csv('../input/hust-ml/train1.csv', dtype=str, encoding='utf-8', usecols=[1])).ravel().tolist()\nx_test = np.array(pd.read_csv('../input/hust-ml/test1.csv', dtype=str, encoding='utf-8', usecols=[0])).ravel().tolist()\ntest_label = [x.strip() for x in codecs.open('../input/hust-ml/ans.txt')]\n\n# news_text = np.array(pd.read_csv('../input/hust-ml/train1.csv', dtype=str, encoding='utf-8', usecols=[0])).ravel().tolist()\n# news_label = np.array(pd.read_csv('../input/hust-ml/train1.csv', dtype=str, encoding='utf-8', usecols=[1])).ravel().tolist()\n# x_train, x_test, train_label, test_label =  train_test_split(news_text[:], news_label[:], test_size=0.2, stratify=news_label[:])\nprint(np.array(x_test).shape)\nprint(np.array(test_label).shape)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2021-11-29T05:40:57.752636Z","iopub.execute_input":"2021-11-29T05:40:57.753092Z","iopub.status.idle":"2021-11-29T05:40:58.708368Z","shell.execute_reply.started":"2021-11-29T05:40:57.752999Z","shell.execute_reply":"2021-11-29T05:40:58.707203Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nimport numpy as np\nimport pandas as pd\nimport random\nimport re\nfrom transformers import BertTokenizer\n# 分词器，词典\n\ntokenizer = BertTokenizer.from_pretrained('../input/huggingface-bert/bert-base-chinese')\ntrain_encoding = tokenizer(x_train, truncation=True, padding=True, max_length=512)\ntest_encoding = tokenizer(x_test, truncation=True, padding=True, max_length=512)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T05:40:58.710525Z","iopub.execute_input":"2021-11-29T05:40:58.710784Z","iopub.status.idle":"2021-11-29T05:44:02.591270Z","shell.execute_reply.started":"2021-11-29T05:40:58.710755Z","shell.execute_reply":"2021-11-29T05:44:02.590273Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"print('Tokenized: ', tokenizer.tokenize(x_train[0]))\nprint('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(x_train[0])))","metadata":{"execution":{"iopub.status.busy":"2021-11-29T05:44:02.594573Z","iopub.execute_input":"2021-11-29T05:44:02.595048Z","iopub.status.idle":"2021-11-29T05:44:02.620539Z","shell.execute_reply.started":"2021-11-29T05:44:02.594988Z","shell.execute_reply":"2021-11-29T05:44:02.619593Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# 数据集读取\nclass NewsDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n    \n    # 读取单个样本\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(int(self.labels[idx]) + 2)\n        return item\n    \n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = NewsDataset(train_encoding, train_label)\ntest_dataset = NewsDataset(test_encoding, test_label)\ntrain_dataset[15]","metadata":{"execution":{"iopub.status.busy":"2021-11-29T05:44:02.623787Z","iopub.execute_input":"2021-11-29T05:44:02.624473Z","iopub.status.idle":"2021-11-29T05:44:02.678322Z","shell.execute_reply.started":"2021-11-29T05:44:02.624407Z","shell.execute_reply":"2021-11-29T05:44:02.677189Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from transformers import BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\nmodel = BertForSequenceClassification.from_pretrained('../input/huggingface-bert/bert-base-chinese', num_labels=4)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# 单个读取到批量读取\ntrain_loader = DataLoader(train_dataset, batch_size=16)\ntest_dataloader = DataLoader(test_dataset, batch_size=16)\n\n# 优化方法\nepoches = 3\nno_decay = ['bias', 'LayerNorm.weight']\noptimizer_grouped_parameters = [\n    # Filter for all parameters which *don't* include 'bias', 'gamma', 'beta'.\n    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.1},\n    \n    # Filter for parameters which *do* include those.\n    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.0}\n]\noptim = AdamW(optimizer_grouped_parameters, lr=2e-5)\ntotal_steps = len(train_loader) * epoches\nscheduler = get_linear_schedule_with_warmup(optim, \n                                            num_warmup_steps = 0, # Default value in run_glue.py\n                                            num_training_steps = total_steps)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T05:44:02.679953Z","iopub.execute_input":"2021-11-29T05:44:02.680937Z","iopub.status.idle":"2021-11-29T05:44:23.778504Z","shell.execute_reply.started":"2021-11-29T05:44:02.680891Z","shell.execute_reply":"2021-11-29T05:44:23.777465Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"params = list(model.named_parameters())\n\nprint('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n\nprint('==== Embedding Layer ====\\n')\n\nfor p in params[0:5]:\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n\nprint('\\n==== First Transformer ====\\n')\n\nfor p in params[5:21]:\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n\nprint('\\n==== Output Layer ====\\n')\n\nfor p in params[-4:]:\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))","metadata":{"execution":{"iopub.status.busy":"2021-11-29T05:44:23.780470Z","iopub.execute_input":"2021-11-29T05:44:23.780819Z","iopub.status.idle":"2021-11-29T05:44:23.801082Z","shell.execute_reply.started":"2021-11-29T05:44:23.780774Z","shell.execute_reply":"2021-11-29T05:44:23.799794Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T05:44:23.803345Z","iopub.execute_input":"2021-11-29T05:44:23.803916Z","iopub.status.idle":"2021-11-29T05:44:23.812437Z","shell.execute_reply.started":"2021-11-29T05:44:23.803869Z","shell.execute_reply":"2021-11-29T05:44:23.811336Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import time\nimport datetime\n\ndef format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    elapsed_rounded = int(round((elapsed)))\n    \n    return str(datetime.timedelta(seconds=elapsed_rounded))","metadata":{"execution":{"iopub.status.busy":"2021-11-29T05:44:23.813767Z","iopub.execute_input":"2021-11-29T05:44:23.814026Z","iopub.status.idle":"2021-11-29T05:44:23.825313Z","shell.execute_reply.started":"2021-11-29T05:44:23.813987Z","shell.execute_reply":"2021-11-29T05:44:23.824317Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# 训练函数\ndef train():\n    model.train()\n    total_train_loss = 0\n    iter_num = 0\n    total_iter = len(train_loader)\n    for batch in train_loader:\n        # 正向传播\n        optim.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs[0]\n        total_train_loss += loss.item()\n        \n        # 反向梯度信息\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        \n        # 参数更新\n        optim.step()\n        scheduler.step()\n\n        iter_num += 1\n        if(iter_num % 100==0):\n            print(\"epoth: %d, iter_num: %d, loss: %.4f, %.2f%%\" % (epoch, iter_num, loss.item(), iter_num/total_iter*100))\n        \n    print(\"Epoch: %d, Average training loss: %.4f\"%(epoch, total_train_loss/len(train_loader)))\n    \ndef validation():\n    model.eval()\n    t0 = time.time()\n    total_eval_accuracy = 0\n    total_eval_loss = 0\n    for batch in test_dataloader:\n        with torch.no_grad():\n            # 正常传播\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        \n        loss = outputs[0]\n        logits = outputs[1]\n\n        total_eval_loss += loss.item()\n        logits = logits.detach().cpu().numpy()\n        label_ids = labels.to('cpu').numpy()\n        total_eval_accuracy += flat_accuracy(logits, label_ids)\n    \n    validation_time = format_time(time.time() - t0)\n    avg_val_accuracy = total_eval_accuracy / len(test_dataloader)\n    print(\"Accuracy: %.4f\" % (avg_val_accuracy))\n    print(\"Average testing loss: %.4f\"%(total_eval_loss/len(test_dataloader)))\n    print(\"Validation took: {:}\".format(validation_time))\n    print(\"-------------------------------\")\n\ndef predict():\n    model.eval()\n    predictions = []\n    for batch in test_dataloader:\n        with torch.no_grad():\n            # 正常传播\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        \n        logits = outputs[1]\n        logits = logits.detach().cpu().numpy()\n        predict = np.argmax(logits, axis=1).flatten()\n        for pred in predict:\n            predictions.append(pred)\n    \n    with open(\"ans.txt\", \"w\") as f:\n        for pred in predictions:\n            f.write(str(pred - 2))\n            f.write('\\n')\n    print(\"-------------------------------\")\n    print(\"predict finished.\")","metadata":{"execution":{"iopub.status.busy":"2021-11-29T05:44:23.827440Z","iopub.execute_input":"2021-11-29T05:44:23.827954Z","iopub.status.idle":"2021-11-29T05:44:23.865470Z","shell.execute_reply.started":"2021-11-29T05:44:23.827906Z","shell.execute_reply":"2021-11-29T05:44:23.864396Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"for epoch in range(4):\n    print(\"------------Epoch: %d ----------------\" % epoch)\n    train()\n    # validation()\n    predict()","metadata":{"execution":{"iopub.status.busy":"2021-11-29T05:44:23.869292Z","iopub.execute_input":"2021-11-29T05:44:23.869847Z","iopub.status.idle":"2021-11-29T08:22:12.968178Z","shell.execute_reply.started":"2021-11-29T05:44:23.869800Z","shell.execute_reply":"2021-11-29T08:22:12.967103Z"},"trusted":true},"execution_count":12,"outputs":[]}]}